{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 训练数据规模大小对与使用CPU和GPU训练的性能的影响"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The overhead of invoking GPU kernels, and copying data to and from GPU, is very high. For operations on models with very little parameters it is not worth of using GPU since frequency of CPU cores is much higher. If you compare matrix multiplication (this is what DL mostly does), you will see that for large matrices GPU outperforms CPU significantly.\n",
    "\n",
    "See [Training a simple model in Tensorflow GPU slower than CPU](https://stackoverflow.com/a/55750666/4627265)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "import time\n",
    "cpu_times = []\n",
    "sizes = [1, 10, 100, 500, 1000, 2000, 3000, 4000, 5000, 8000, 10000]\n",
    "for size in sizes:\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    start = time.time()\n",
    "    with tf.device('cpu:0'):\n",
    "        v1 = tf.Variable(tf.compat.v1.random_normal((size, size)))\n",
    "        v2 = tf.Variable(tf.compat.v1.random_normal((size, size)))\n",
    "        op = tf.matmul(v1, v2)\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(op)\n",
    "    cpu_times.append(time.time() - start)\n",
    "    print('cpu time took: {0:.4f}'.format(time.time() - start))\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "gpu_times = []\n",
    "for size in sizes:\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    start = time.time()\n",
    "    with tf.device('gpu:0'):\n",
    "        v1 = tf.Variable(tf.compat.v1.random_normal((size, size)))\n",
    "        v2 = tf.Variable(tf.compat.v1.random_normal((size, size)))\n",
    "        op = tf.matmul(v1, v2)\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(op)\n",
    "    gpu_times.append(time.time() - start)\n",
    "    print('gpu time took: {0:.4f}'.format(time.time() - start))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(sizes, gpu_times, label='GPU')\n",
    "ax.plot(sizes, cpu_times, label='CPU')\n",
    "plt.xlabel('MATRIX SIZE')\n",
    "plt.ylabel('TIME (sec)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
